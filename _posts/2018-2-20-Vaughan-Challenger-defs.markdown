---
layout: post
title:  "Vaughan - The Challenger Launch Decision"
date:   2018-2-19 17:50:00
categories: ['Vaugan','Culture QE','Reading Notes']
published: true
Abstract:
defs_used:


---

Chapters 2 3,10

# Chapter 2

Why do organizations make bad decisions? Economic strain might be part of it (amoral calculation - people look at their financial situation and do things based on money without respect for the law), but we know that not all organizations violate laws and rules to attain goals. Also, bad decisions can come from other places.

Vaughan thought at first that maybe amoral calculation played a part, but it is a strawman. NASA seems to have a history of doing cost/safety trade-offs and of having meetings where the decision seemed to already be made.

But there was all sorts o fthings going on, and two of them were:
1. In the decision making progress, we began to see something called "acceptable risk" in official decisions.
2. The rules followed by the challenger team were similar to those at NASA.

>The attention
paid to managers and rule violations after the disaster deflected
attention from the compelling fact that, in the years preceding the
Challenger launch, engineers and managers together developed a definition
of the situation that allowed them to carry on as if nothinb
was wrong when they continually faced evidence that somethinb
was wrong. This is the problem of the normalization of deviance.
Three factors, with which we will be occupied throughout this book,
explain the normalization of deviance: the production of a work
group culture, the culture of production, and structural secrecy. p. 62

How did deviance become normalized at NASA?

We kind of have to study how organizational culture happens? We don't really believe a priori that there is one cuture, we need to study organizations to understand how culture works at that level.

Organizational culture - rules, rituals and beliefs that are unique to work groups.

>People in an organization who interact because the,
have a central task in common constitute a work group. The common
task draws them together, and in the doing of that task lies tho:
potential for development of a culture unique to that particular task p. 64

In an orgaization, decision making becomes patterned, notions become firmly set.

>Patterns oi
the past-in this case, decision-making patterns pertaining to technical
components-constitute part of the social context of decisior.
making in the present. This decision-making pattern indicates the
development of norms, procedures, and beliefs that characterized the
work group culture. p. 66

We want to know why the normalization of this decision pattern mattered?

Moreover, the people who understood the risk spread the culture of risk to those who understoo i t less.

# Risk, Work Group Culture, and the Normalization of Deviance

As they saw in tests that the o-rings were getting damaged, rather than fix the problem they deemed the risk from it as increasingly acceptable. That is, deviance from the norm became aceptable.

We try to uncover culture here by examining what people do and say.

WE know that the work group who worked on the challenger worked together to develop an understanding of the situation.  

There was a lot of technological uncertainty because the thing they were making was new.

In work groups, they come up with something called the acceptable risk process - which amde its way fram daily deiciosn making into formalized final deicisons.
>Starting from the assumption
that all shuttle components are risky, engineers had to determine
whether the risk of each item was acceptable. II Acceptable
Risk" was a formal status conferred on a component by following a
prescribed NASA procedure. When engineers discovered an anomaly
or questionable condition, it was treated as a signal of potential danger.
The shuttle could not fly unless the hazard was eliminated or
controlled by some corrective action and/or engineering calculations
and tests establishing that the condition was not a threat to flight
safety. It could be classified as an Acceptable Risk only on the basis
of a documented engineering risk rationale (in hearings testimony
and interview transcripts of work group participants often called
simply a "technical rationale," or IIrationale"; another usage was
IIwe rationalized it"). A rationale was an analysis of the problem, the
probability of its recurrence, and data supporting a conclusion of
acceptable risk. p. 81

Then, during discussion the acceptable risk makes it into each part of the discussion.

There were also different work groups with different roles that became routinized, some looking for mistakes. Studies between two main scientists often conflicted. Even the same results could be interpreted differently.
>To resolve disagreement based on controversies about types of
test and/or equipment differences between Thiokol and Marshall, the
same result confirmed by multiple methods, multiple tests, and the
most rigorous engineering analysis won the day. Consensus was the
goal p.87

Divisions within the group also created disagreement. They were presented symbolically as different sides of the house, cooperative adversaries. Usually, again, data prevailed, but sometimes it iddn't.

>dIn case of an unresolvable conflict
between the design needs, as defined by the purists, insufficient data
to create consensus, and the cost and schedule concerns of the Project
Manager, a "management risk decision" was made. In NASA
culture, this term was not pejorative, but prosaic, referring to the
routine weighing of cost, schedule, and safety in management decisions
when the data were not sufficiently compelling to create consensus. p.89

Disputes were moved up as time went on, but resolved in the same way.

>Disputes
in FRR were settled by the same methods as daily disputes between
the two engineering communities or between Project Management
and S&E personnel: the methods of positivistic science, emphasis on
quantitative analysis, and multiple tests with data covering every
known condition were the means by which uncertainty was converted
to certainty. To resolve them, Project Managers assigned
"Action Items" directing that more research, more tests, more data
analysis be done to settle these issues. "Delta" reviews were additional,
follow-up meetings called to check out remaining open technical
issues and review results of Action Items. The problem, of
course, was that for the Space Shuttle, all conditions could never be
known p. 91

>From the bottom-up negotiation in FRR, an official construction
of risk resulted, certifying all shuttle components as flight-ready,
acceptable risks. However, as countdown proceeded to its conclusion,
the engineers most familiar with the technology were well
aware of foibles of the technology and the negotiated nature of the
aggregate residual risk of the shuttle. p. 95

Vaughan then shows how discussions about the O-ring from the start were part of this task-based organizational culture.

>To determine whether the design was an acceptable
risk, the working engineers analyzed SRB joint performance in the
three areas in which their design was violating recommended industry
standards: (1) sealing by extrusion, not compression, (2) gap size,
and (3) initial compression level. They did "extrusion tests" to examine
the primary's ability to seal with gaps much larger than either the
Marshall or Thiokol figures on gap size. With a gap of 0.125" (an
eighth of an inch-about twice Marshall's assessment) and using
5,000 psi (pounds per square inch; five times the pressure per square
inch experienced during launch), they had no problem with the seal.
Said Ray, "We opened the gap up that far (0.125") and pressurized up
to 5,000 psi and everything was okay."83 Both Marshall and Thiokol
now felt confident in the primary's ability to seal. They found that
despite its being in violation of the industry guidelines for gap size
and sealing mechanism, the primary ring sealed under conditions
far more severe than anything likely to be experienced during a
launch. Once the primary sealed, and the ignition transient completed,
ending joint rotation, the primary would remain tightly
wedged in the now-closed gap through the two-minute period until
the boosters were jettisoned into the sea. p. 103

We also see evidence in these early years that early decisions become guidelines for ways that people think down the line.

Basically, even though they knew there was a potential for catastrophic failure, the risk became normalized over time through a series of assumptions.

An initial launch seemed to prove them right and they began to

>Although in these early history
chapters we focus on the microcosmic wo~ld of routine decisions to
show the process of culture production and how the work group normalized
technical deviation, the very obvious influences from the
external environment and the NASA organization warrant some discussion
now. They sensitize us to why the normalization of deviance
occurred. I identify some of these structural influences briefly in
order to establish them as a context for the next two chapters p. 114

## Lessons Learned
People blamed middle managers but there was more:
1. NASA's elites built a competitive organizational environment that perhaps hindered good calculations of risk.
2. NASA also allowed space to become routine, and this forced them to allow a teacher on board.

But finally,
3. There is a difficulty in making engineering decisions about shuttle technology. There was a role here for daily engineering routine, working people and tehcnology. They turned things they did not understand into acceptable risks.

>Most serious was the discovery that misunderstanding about the
most critical technical rule used had contributed to a flawed decision.
The temperature specifications had been misread, misinterpreted,
and misused by both Thiokol and Marshall for years. The
31°F-99°F ambient temperature requirement for launching the entire
shuttle system was created early in the program by other engineers.
In a palpable demonstration of structural secrecy, many teleconference
participants had jobs that did not require them to know Cape
launch requirements and so were unaware that this temperature
specification existed.1O More profoundly disturbing, however, was
that for Mulloy, Hardy, and many others who used it regularly in
launch decision making at the Cape, this launch decision rule had
become dissociated from its creators and the engineering process
behind its creation. They had followed it repeatedly, taking for granted
the interpretative work that other engineers had done. p.391


>What is important to remember from this case is not that individuals
in organizations make mistakes, but that mistakes themselve.,
are socially organized and systematically produced. Contradicting
the rational choice theory behind the hypothesis of managers a3
amoral calculators, the tragedy had systemic origins that transcended
individuals, organization, time, and geography. Its sources were
neither extraordinary nor necessarily peculiar to NASA, as the
amoral calculator hypothesis would lead us to believe. Instead, it3
origins were in routine and taken-for-granted aspects of organizational1i£
e that created a way of seeing that was simultaneously a way
of not seeing. The normalization of deviant joint performance is the
answer to both questions raised at the beginning of this book: Why
did NASA continue to launch shuttles prior to 1986 with a design
that was not performing as predicted? Why was the Challenger
launched over the objections of engineers? p. 394


As discussion went on, thee engineering principles were applied at different points in the process.

>The capstone, of course,
was the feedback from the postflight engineering analysis that
showed that their prelaunch corrective actions and performance predictions
in response to the most recent anomaly had been correct
Alone, however, the social affirmation and commitment generatec
as work group decisions were processed are insufficient to explai~
the normalization of deviance. The culture of production and structural
secrecy were environmental and organizational contingencies
that caused the work group culture to persist. p.396

>The workplace was dominated by three cultural
imperatives: the original technical culture, bureaucratic accountabil·
ity, and political accountability. We saw how NASA's environmen:
changed, so that the original technical culture struggled to survive
amid institutionalized production concerns and bureaupathology. We
saw how the performance pressure at Marshall mandated conformity
to all three cultural imperatives, and that the work group did, in fact conofrm.

Why did workgroups normalize risk that they normally wouldn't? There was some structural secrecy, it failed to spread important information.

>Signals of potential danger were embedded in patterns of information
that affected the work group's definition of the situation. Signals
lost their salience as a result of the risk assessment process. p.397

>In language that staggers in its seeming prescience about events at
NASA, Kuhn describes normal science as "an attempt to force nature
into the preformed and relatively inflexible box that the paradigm supplies.
No part of the aim of normal science is to call forth new sorts of
phenomena; indeed those that will not fit the box are often not seen
at all."30 According to Kuhn, the recalcitrance of a scientific paradigm
is in the worldview it engenders; it is overturned when a crisis arises
that precipitates a transformation of that worldview p.401

>But individuals were not passive recipients of culture. The case
shows how people at all hierarchical levels actively participated in its
production and reproduction.p.404


>Possibly the most significant
lesson from the Challenger case is how environmental and organizational
contingencies create prerational forces that shape worldview,
normalizing signals of potential danger, resulting in mistakes with
harmful human consequences. p. 409

We see in science people only wanting to look at the data that they want to see.
